spring.application.name=llm-chat-service-test

server.port=0

# Actuator endpoints
management.endpoints.web.exposure.include=health,info
management.endpoint.health.show-details=always

# Ollama LLM Configuration (test values)
llm.ollama.base-url=http://localhost:8089
llm.ollama.model=test-model
llm.ollama.temperature=0.5
llm.ollama.max-tokens=1000
llm.ollama.timeout-seconds=30

# WebClient Configuration
spring.codec.max-in-memory-size=10MB

# Logging Configuration (less verbose for tests)
logging.level.de.jivz.llmchatservice=INFO
logging.level.org.springframework.web=WARN
logging.level.reactor.netty.http.client=WARN
