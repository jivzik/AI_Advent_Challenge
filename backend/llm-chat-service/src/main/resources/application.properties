spring.application.name=llm-chat-service

# Prefer IPv4 to avoid DNS resolution delays
spring.main.prefer-ipv4-stack=true

server.port=8090
server.servlet.context-path=/

# Actuator endpoints
management.endpoints.web.exposure.include=health,info
management.endpoint.health.show-details=always
management.health.defaults.enabled=true

# Ollama LLM Configuration
llm.ollama.base-url=${OLLAMA_BASE_URL:http://localhost:11434}
llm.ollama.model=${OLLAMA_MODEL:phi3:mini}
llm.ollama.temperature=${OLLAMA_TEMPERATURE:0.7}
llm.ollama.max-tokens=${OLLAMA_MAX_TOKENS:2000}
llm.ollama.timeout-seconds=${OLLAMA_TIMEOUT:120}

# WebClient Configuration
spring.webflux.base-path=/
spring.codec.max-in-memory-size=10MB

# Logging Configuration
logging.level.de.jivz.llmchatservice=DEBUG
logging.level.org.springframework.web=INFO
logging.level.reactor.netty.http.client=DEBUG
logging.pattern.console=%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n
