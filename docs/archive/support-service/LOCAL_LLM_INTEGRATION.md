# Local LLM Integration (Ollama)

## √úbersicht

Der Support-Service unterst√ºtzt jetzt sowohl **lokale LLM** (Ollama) als auch **remote LLM** (OpenRouter). Der Benutzer kann im Frontend zwischen den beiden Optionen wechseln.

## Architektur

### Backend-Komponenten

1. **OllamaProperties** (`config/OllamaProperties.java`)
   - Konfigurationsklasse f√ºr Ollama-Einstellungen
   - Properties mit Prefix `llm.ollama`

2. **OllamaWebClientConfig** (`config/OllamaWebClientConfig.java`)
   - WebClient-Konfiguration f√ºr Ollama API
   - Connection Pooling, Timeouts, Error Handling

3. **OllamaApiClient** (`service/client/OllamaApiClient.java`)
   - Client f√ºr Ollama API-Kommunikation
   - Analog zu OpenRouterApiClient

4. **ToolExecutionOrchestrator** (erweitert)
   - Unterst√ºtzt Provider-Parameter (`local` oder `remote`)
   - W√§hlt automatisch den richtigen Client

5. **SupportChatService** (erweitert)
   - Nimmt `llmProvider` aus dem Request
   - √úbergibt Provider an ToolExecutionOrchestrator

### Frontend-Komponenten

1. **SupportChat.vue** (erweitert)
   - Toggle-Button f√ºr LLM Provider
   - State: `llmProvider` ('local' | 'remote')
   - Sendet Provider-Pr√§ferenz an Backend

2. **supportChatService.ts** (erweitert)
   - Interface `SendMessageRequest` erweitert um `llmProvider`

## Konfiguration

### application.properties

```properties
# Ollama Local LLM Configuration
llm.ollama.base-url=http://localhost:11434
llm.ollama.model=gemma2:2b
llm.ollama.temperature=0.7
llm.ollama.max-tokens=1000
llm.ollama.timeout-seconds=120
```

### Ollama starten

```bash
# Ollama installieren (falls noch nicht geschehen)
# https://ollama.ai/download

# Modell herunterladen
ollama pull gemma2:2b

# Ollama l√§uft automatisch im Hintergrund
# API ist unter http://localhost:11434 verf√ºgbar
```

## Verwendung

### Frontend

1. √ñffne Support Chat
2. Klicke auf den Provider-Toggle-Button im Header
3. Wechsle zwischen:
   - **‚òÅÔ∏è Remote (OpenRouter)** - Verwendet Claude 3.5 Sonnet
   - **ü§ñ Local (Ollama)** - Verwendet gemma2:2b

### API Request

```json
{
  "userEmail": "user@example.com",
  "message": "Wie funktioniert die Authentifizierung?",
  "llmProvider": "local"
}
```

## Vorteile

### Remote LLM (OpenRouter)
- ‚úÖ Leistungsst√§rkere Modelle (Claude 3.5 Sonnet)
- ‚úÖ Bessere Tool-Verwendung und Reasoning
- ‚úÖ Keine lokale Hardware erforderlich
- ‚ùå Erfordert API-Key und Internet
- ‚ùå Kosten pro Request

### Local LLM (Ollama)
- ‚úÖ Keine Kosten
- ‚úÖ Datenschutz (Daten bleiben lokal)
- ‚úÖ Offline-Betrieb m√∂glich
- ‚úÖ Schnelle Antworten (kein Netzwerk-Overhead)
- ‚ùå Ben√∂tigt lokale GPU/CPU-Ressourcen
- ‚ùå Kleineres Modell (gemma2:2b)

## Testing

### Backend kompilieren

```bash
cd backend/support-service
mvn clean compile
```

### Service starten

```bash
cd backend/support-service
mvn spring-boot:run
```

### Frontend starten

```bash
cd frontend
npm run dev
```

### Test im Browser

1. √ñffne http://localhost:5173
2. Navigiere zu Support Chat
3. Teste beide Provider-Modi:
   - Stelle eine Frage mit Remote Provider
   - Wechsle zu Local Provider
   - Stelle dieselbe Frage erneut
   - Vergleiche die Antworten

## Troubleshooting

### Ollama Connection Error

**Problem:** `Failed to call Ollama API`

**L√∂sung:**
```bash
# Pr√ºfe ob Ollama l√§uft
curl http://localhost:11434/api/version

# Starte Ollama neu
ollama serve
```

### Modell nicht gefunden

**Problem:** `model 'gemma2:2b' not found`

**L√∂sung:**
```bash
# Modell herunterladen
ollama pull gemma2:2b

# Verf√ºgbare Modelle anzeigen
ollama list
```

### Timeout Errors

**Problem:** Anfragen laufen in Timeout

**L√∂sung:**
- Erh√∂he `llm.ollama.timeout-seconds` in application.properties
- Verwende ein kleineres Modell
- Reduziere `llm.ollama.max-tokens`

## Weitere Modelle

### Andere Ollama Modelle verwenden

```properties
# Llama 2 (7B)
llm.ollama.model=llama2

# Mistral (7B)
llm.ollama.model=mistral

# CodeLlama (f√ºr Code-Fragen)
llm.ollama.model=codellama

# Gemma 2 (9B - gr√∂√üeres Modell)
llm.ollama.model=gemma2:9b
```

Modell √§ndern und Service neu starten.

## Performance-Tipps

1. **Kleineres Modell f√ºr schnellere Antworten**: `gemma2:2b`
2. **Gr√∂√üeres Modell f√ºr bessere Qualit√§t**: `gemma2:9b` oder `llama2`
3. **GPU verwenden** f√ºr deutlich schnellere Inference
4. **Temperature reduzieren** (0.3-0.5) f√ºr deterministischere Antworten
5. **max-tokens limitieren** f√ºr schnellere Responses

## N√§chste Schritte

- [ ] Provider-Pr√§ferenz im LocalStorage speichern
- [ ] Performance-Metriken anzeigen (Response Time, Tokens/s)
- [ ] Modell-Auswahl im Frontend erm√∂glichen
- [ ] Streaming-Support f√ºr Ollama
- [ ] Fehlerbehandlung verbessern

